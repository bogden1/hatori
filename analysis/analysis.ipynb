{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import nltk.tokenize\n",
    "import nltk.corpus\n",
    "import nltk.stem\n",
    "\n",
    "import gensim.models.wrappers\n",
    "import gensim.parsing.preprocessing\n",
    "import gensim\n",
    "\n",
    "import time\n",
    "import string\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_pos = {\n",
    "    \"FW\",\n",
    "    \"JJ\",\n",
    "    \"JJR\",\n",
    "    \"JJS\",\n",
    "    \"NN\",\n",
    "    \"NNS\",\n",
    "    \"NP\",\n",
    "    \"NPS\",\n",
    "    \"RB\",\n",
    "    \"RBR\",\n",
    "    \"RBS\",\n",
    "    \"VB\",\n",
    "    \"VBD\",\n",
    "    \"VBG\",\n",
    "    \"VBN\",\n",
    "    \"VBP\",\n",
    "    \"VBZ\",\n",
    "}\n",
    "\n",
    "wordnet_lemmatizer = nltk.stem.SnowballStemmer(\"english\")\n",
    "stops = (set(nltk.corpus.stopwords.words('english')) \n",
    "       | set(gensim.parsing.preprocessing.STOPWORDS) \n",
    "       | set(string.punctuation))\n",
    "any_forbidden = set(\"0123456789*+\")\n",
    "all_forbidden = set(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debate_tokenize(text):\n",
    "    text = text.replace(\".\", \". \") # happens often enough that it becomes a problem\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "    poses = [wordnet_lemmatizer.stem(w).lower()\n",
    "                 for w, p in nltk.pos_tag(tokens)\n",
    "                 if p in relevant_pos]\n",
    "    tokens = [w for w in poses \n",
    "                if w not in stops\n",
    "                and not any(c in any_forbidden for c in w)\n",
    "                and not all(c in all_forbidden for c in w)]\n",
    "    return tokens\n",
    "\n",
    "def stemmer(index, key):\n",
    "    namedate, _, text = key.split(\"\\t\")\n",
    "    tokens = debate_tokenize(text)\n",
    "    name, date = namedate[:10], namedate[10:]\n",
    "    return (index, name, date, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pool = multiprocessing.Pool(processes=2)\n",
    "with open(\"data/hansard_debates.tsv\") as stream:\n",
    "    # subdata = [next(stream) for i in range(5000)]\n",
    "    results = [pool.apply_async(stemmer, v) for v in enumerate(stream)]\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 150928 25.18673611911309            \r"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-b7033e3de8ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mnum_done\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_done\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_done\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m# avg = (num_done-start_num) / (time.time() - start_time)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"\\r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-86-b7033e3de8ca>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mnum_done\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_done\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_done\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m# avg = (num_done-start_num) / (time.time() - start_time)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"\\r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "num_done = sum(results[i].ready() for i in range(len(results)))\n",
    "start_num = num_done\n",
    "start_time = time.time()\n",
    "for i in range(0):\n",
    "    time.sleep(0.5)\n",
    "    num_done += sum(results[i].ready() for i in range(num_done, num_done+50))\n",
    "    avg = (num_done-start_num) / (time.time() - start_time)\n",
    "    print(i, num_done, avg, end=\" \"*12+\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all(i.ready() for i in results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [i.get() for i in results]\n",
    "with open(\"data/stemmed.p\", \"wb\") as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/stemmed.p\", \"rb\") as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..................................................  5000\n",
      "..................................................  10000\n",
      "..................................................  15000\n",
      "..................................................  20000\n",
      "..................................................  25000\n",
      "..................................................  30000\n",
      "..................................................  35000\n",
      "..................................................  40000\n",
      "..................................................  45000\n",
      "..................................................  50000\n",
      "..................................................  55000\n",
      "..................................................  60000\n",
      "..................................................  65000\n",
      "..................................................  70000\n",
      "..................................................  75000\n",
      "..................................................  80000\n",
      "..................................................  85000\n",
      "..................................................  90000\n",
      "..................................................  95000\n",
      "..................................................  100000\n",
      "..................................................  105000\n",
      "..................................................  110000\n",
      "..................................................  115000\n",
      "..................................................  120000\n",
      "..................................................  125000\n",
      "..................................................  130000\n",
      "..................................................  135000\n",
      "..................................................  140000\n",
      "..................................................  145000\n",
      "..................................................  150000\n",
      "........"
     ]
    }
   ],
   "source": [
    "counts = {}\n",
    "counter = 0\n",
    "for _, _, _, l in data:\n",
    "    for w in l:\n",
    "        counts[w] = counts.get(w, 0) + 1\n",
    "    counter += 1\n",
    "    if counter % 100 == 0:\n",
    "            print(\".\", end=\"\")\n",
    "    if counter % 5000 == 0:\n",
    "        print(\" \", counter)\n",
    "sorted([(v, k) for k, v in counts.items()], reverse=True)[:30]\n",
    "excluded = {\"hon\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = [(i, date, name, [w for w in l \n",
    "                if len(w) > 3\n",
    "                and counts[w] >= 200 \n",
    "                and w not in excluded]) \n",
    "            for (i, date, name, l) in data]\n",
    "filtered = [(i, date, name, l) for (i, date, name, l) in filtered if len(l) > 100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist = {w for *_, l in filtered for w in l}\n",
    "idmap    = sorted(wordlist)\n",
    "wordmap  = {w:i for i, w in enumerate(idmap)}\n",
    "filtered = [(i, date, name, [wordmap[w] for w in l]) for (i, date, name, l) in filtered]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/filtered.p\", \"wb\") as f:\n",
    "    pickle.dump({\"idmap\" : idmap, \"filtered\" : filtered}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/filtered.p\", \"rb\") as f:\n",
    "    d = pickle.load(f)\n",
    "    filtered = d[\"filtered\"]\n",
    "    idmap    = d[\"idmap\"]\n",
    "    wordmap  = {w:i for i, w in enumerate(idmap)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = {}\n",
    "counter = 0\n",
    "for *_, l in filtered:\n",
    "    for w in l:\n",
    "        counts[w] = counts.get(w, 0) + 1\n",
    "    counter += 1\n",
    "    if counter % 100 == 0:\n",
    "            print(\".\", end=\"\")\n",
    "    if counter % 5000 == 0:\n",
    "        print(\" \", counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [[idmap[w] for w in l] for *_, l in filtered]\n",
    "dictionary = gensim.corpora.Dictionary(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-7ef4e1535a78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0moptimize_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0miterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     workers = 1)\n\u001b[0m",
      "\u001b[0;32m~/learn/lib/python3.7/site-packages/gensim/models/wrappers/ldamallet.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, mallet_path, corpus, num_topics, alpha, id2word, workers, prefix, optimize_interval, iterations, topic_threshold)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfinferencer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/learn/lib/python3.7/site-packages/gensim/models/wrappers/ldamallet.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, corpus)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \"\"\"\n\u001b[0;32m--> 267\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mcmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmallet_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' train-topics --input %s --num-topics %s  --alpha %s --optimize-interval %s '\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0;34m'--num-threads %s --output-state %s --output-doc-topics %s --output-topic-keys %s '\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/learn/lib/python3.7/site-packages/gensim/models/wrappers/ldamallet.py\u001b[0m in \u001b[0;36mconvert_input\u001b[0;34m(self, corpus, infer, serialize_corpus)\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"serializing temporary corpus to %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfcorpustxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0msmart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfcorpustxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus2mallet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;31m# convert the text file above into MALLET's internal format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/learn/lib/python3.7/site-packages/gensim/models/wrappers/ldamallet.py\u001b[0m in \u001b[0;36mcorpus2mallet\u001b[0;34m(self, corpus, file_like)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdocno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2word\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m                 \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokenid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtokenid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtokenid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/learn/lib/python3.7/site-packages/gensim/models/wrappers/ldamallet.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdocno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2word\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m                 \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokenid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtokenid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtokenid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lda_model = gensim.models.wrappers.LdaMallet(\n",
    "    \"/usr/local/bin/mallet\",\n",
    "    corpus  = bow_corpus, \n",
    "    id2word = dictionary, \n",
    "    num_topics = 200,\n",
    "    optimize_interval=20,\n",
    "    iterations = 30000,\n",
    "    workers = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['b',\n",
       " 'c',\n",
       " 'e',\n",
       " 'f',\n",
       " 'h',\n",
       " 'j',\n",
       " 'l',\n",
       " 'n',\n",
       " 'w',\n",
       " \"'s\",\n",
       " 'a.',\n",
       " 'ab',\n",
       " 'ad',\n",
       " 'ah',\n",
       " 'al',\n",
       " 'au',\n",
       " 'aw',\n",
       " 'ay',\n",
       " 'c.',\n",
       " 'd.',\n",
       " 'e.',\n",
       " 'ed',\n",
       " 'en',\n",
       " 'ep',\n",
       " 'er',\n",
       " 'et',\n",
       " 'ex',\n",
       " 'ft',\n",
       " 'h.',\n",
       " 'ha',\n",
       " 'hi',\n",
       " 'ho',\n",
       " 'i.',\n",
       " 'ii',\n",
       " 'l.',\n",
       " 'la',\n",
       " 'lb',\n",
       " 'le',\n",
       " 'lo',\n",
       " 'm.',\n",
       " 'mo',\n",
       " 'ne',\n",
       " 'oh',\n",
       " 'oz',\n",
       " 'p.',\n",
       " 'pf',\n",
       " 'pp',\n",
       " 'r.',\n",
       " 'rt',\n",
       " 's.',\n",
       " 'se',\n",
       " 'su',\n",
       " 'tu',\n",
       " 'v.',\n",
       " 'ye',\n",
       " 'abl',\n",
       " 'ac-',\n",
       " 'acr',\n",
       " 'act',\n",
       " 'ad-',\n",
       " 'add',\n",
       " 'age',\n",
       " 'ago',\n",
       " 'aid',\n",
       " 'ail',\n",
       " 'aim',\n",
       " 'air',\n",
       " 'al-',\n",
       " 'ale',\n",
       " 'alm',\n",
       " 'an-',\n",
       " 'anc',\n",
       " 'ani',\n",
       " 'ant',\n",
       " 'ap-',\n",
       " 'apt',\n",
       " 'arc',\n",
       " 'arm',\n",
       " 'art',\n",
       " 'as-',\n",
       " 'ash',\n",
       " 'ask',\n",
       " 'ass',\n",
       " 'at-',\n",
       " 'ate',\n",
       " 'awe',\n",
       " 'axe',\n",
       " 'aye',\n",
       " 'bad',\n",
       " 'bag',\n",
       " 'ban',\n",
       " 'bar',\n",
       " 'bat',\n",
       " 'bay',\n",
       " 'be-',\n",
       " 'be.',\n",
       " 'bed',\n",
       " 'bee',\n",
       " 'beg',\n",
       " 'ber',\n",
       " 'bet',\n",
       " 'bid',\n",
       " 'big',\n",
       " 'bit',\n",
       " 'bog',\n",
       " 'bon',\n",
       " 'bow',\n",
       " 'box',\n",
       " 'boy',\n",
       " 'bud',\n",
       " 'buy',\n",
       " 'bye',\n",
       " 'cab',\n",
       " 'cap',\n",
       " 'car',\n",
       " 'cat',\n",
       " 'com',\n",
       " 'cow',\n",
       " 'cri',\n",
       " 'cum',\n",
       " 'cun',\n",
       " 'cup',\n",
       " 'cur',\n",
       " 'cut',\n",
       " 'cwt',\n",
       " 'dam',\n",
       " 'day',\n",
       " 'de-',\n",
       " 'deg',\n",
       " 'den',\n",
       " 'des',\n",
       " 'die',\n",
       " 'dig',\n",
       " 'dim',\n",
       " 'dip',\n",
       " 'do.',\n",
       " 'doc',\n",
       " 'doe',\n",
       " 'dog',\n",
       " 'dri',\n",
       " 'dug',\n",
       " 'duo',\n",
       " 'dye',\n",
       " 'eal',\n",
       " 'ear',\n",
       " 'eas',\n",
       " 'eat',\n",
       " 'ebb',\n",
       " 'edg',\n",
       " 'egg',\n",
       " 'eke',\n",
       " 'els',\n",
       " 'em-',\n",
       " 'en-',\n",
       " 'enc',\n",
       " 'end',\n",
       " 'era',\n",
       " 'ere',\n",
       " 'err',\n",
       " 'esq',\n",
       " 'est',\n",
       " 'eve',\n",
       " 'ex-',\n",
       " 'eye',\n",
       " 'fad',\n",
       " 'fag',\n",
       " 'fan',\n",
       " 'far',\n",
       " 'fat',\n",
       " 'fed',\n",
       " 'fee',\n",
       " 'fer',\n",
       " 'feu',\n",
       " 'fie',\n",
       " 'fit',\n",
       " 'fix',\n",
       " 'fli',\n",
       " 'foe',\n",
       " 'fog',\n",
       " 'fox',\n",
       " 'fro',\n",
       " 'ft.',\n",
       " 'ful',\n",
       " 'fur',\n",
       " 'gag',\n",
       " 'gap',\n",
       " 'gas',\n",
       " 'gin',\n",
       " 'go-',\n",
       " 'god',\n",
       " 'goe',\n",
       " 'got',\n",
       " 'gun',\n",
       " 'ham',\n",
       " 'har',\n",
       " 'hat',\n",
       " 'hay',\n",
       " 'hem',\n",
       " 'hen',\n",
       " 'hid',\n",
       " 'hit',\n",
       " 'hoc',\n",
       " 'hop',\n",
       " 'hot',\n",
       " 'hut',\n",
       " 'ice',\n",
       " 'idl',\n",
       " 'ill',\n",
       " 'im-',\n",
       " 'in-',\n",
       " 'ing',\n",
       " 'ink',\n",
       " 'inn',\n",
       " 'ion',\n",
       " 'ips',\n",
       " 'isl',\n",
       " 'it.',\n",
       " 'jam',\n",
       " 'jar',\n",
       " 'jew',\n",
       " 'job',\n",
       " 'jot',\n",
       " 'joy',\n",
       " 'key',\n",
       " 'kin',\n",
       " 'kit',\n",
       " 'lac',\n",
       " 'lad',\n",
       " 'lag',\n",
       " 'law',\n",
       " 'lax',\n",
       " 'lay',\n",
       " 'lbs',\n",
       " 'led',\n",
       " 'leg',\n",
       " 'les',\n",
       " 'let',\n",
       " 'lex',\n",
       " 'lie',\n",
       " 'lip',\n",
       " 'lit',\n",
       " 'log',\n",
       " 'lop',\n",
       " 'lot',\n",
       " 'low',\n",
       " 'ma-',\n",
       " 'mad',\n",
       " 'man',\n",
       " 'map',\n",
       " 'mar',\n",
       " 'mat',\n",
       " 'me.',\n",
       " 'men',\n",
       " 'met',\n",
       " 'mid',\n",
       " 'mit',\n",
       " 'mix',\n",
       " 'mob',\n",
       " 'mon',\n",
       " 'mot',\n",
       " 'mr.',\n",
       " 'mud',\n",
       " \"n't\",\n",
       " 'nay',\n",
       " 'net',\n",
       " 'new',\n",
       " 'nil',\n",
       " 'no.',\n",
       " 'nod',\n",
       " 'non',\n",
       " 'nun',\n",
       " 'nut',\n",
       " 'oak',\n",
       " 'oar',\n",
       " 'oas',\n",
       " 'oat',\n",
       " 'ob-',\n",
       " 'odd',\n",
       " 'oft',\n",
       " 'oil',\n",
       " 'old',\n",
       " 'onc',\n",
       " 'op-',\n",
       " 'ore',\n",
       " 'owe',\n",
       " 'par',\n",
       " 'pay',\n",
       " 'pea',\n",
       " 'peg',\n",
       " 'pen',\n",
       " 'pet',\n",
       " 'pew',\n",
       " 'pig',\n",
       " 'pin',\n",
       " 'pit',\n",
       " 'pli',\n",
       " 'pot',\n",
       " 'pox',\n",
       " 'pre',\n",
       " 'pro',\n",
       " 'qua',\n",
       " 'que',\n",
       " 'qui',\n",
       " 'quo',\n",
       " 'rag',\n",
       " 'ram',\n",
       " 'ran',\n",
       " 'rat',\n",
       " 'raw',\n",
       " 'ray',\n",
       " 're-',\n",
       " 'red',\n",
       " 'rev',\n",
       " 'rid',\n",
       " 'rig',\n",
       " 'rob',\n",
       " 'rod',\n",
       " 'rot',\n",
       " 'row',\n",
       " 'rub',\n",
       " 'rue',\n",
       " 'rum',\n",
       " 'run',\n",
       " 'sad',\n",
       " 'sap',\n",
       " 'sat',\n",
       " 'saw',\n",
       " 'sea',\n",
       " 'sec',\n",
       " 'set',\n",
       " 'sex',\n",
       " 'shi',\n",
       " 'sin',\n",
       " 'sir',\n",
       " 'sit',\n",
       " 'sky',\n",
       " 'so.',\n",
       " 'son',\n",
       " 'sop',\n",
       " 'sot',\n",
       " 'sow',\n",
       " 'spi',\n",
       " 'sub',\n",
       " 'sue',\n",
       " 'sum',\n",
       " 'sun',\n",
       " 'sup',\n",
       " 'tap',\n",
       " 'tar',\n",
       " 'tax',\n",
       " 'tea',\n",
       " 'ter',\n",
       " 'thi',\n",
       " 'tie',\n",
       " 'tin',\n",
       " 'tip',\n",
       " 'to.',\n",
       " 'ton',\n",
       " 'tow',\n",
       " 'toy',\n",
       " 'tri',\n",
       " 'tug',\n",
       " 'ult',\n",
       " 'un-',\n",
       " 'up.',\n",
       " 'urg',\n",
       " 'us.',\n",
       " 'use',\n",
       " 'van',\n",
       " 'vet',\n",
       " 'vex',\n",
       " 'vie',\n",
       " 'viz',\n",
       " 'vol',\n",
       " 'vow',\n",
       " 'wag',\n",
       " 'war',\n",
       " 'wax',\n",
       " 'way',\n",
       " 'wed',\n",
       " 'wet',\n",
       " 'wil',\n",
       " 'win',\n",
       " 'wit',\n",
       " 'woe',\n",
       " 'yes',\n",
       " 'yon',\n",
       " 'abat',\n",
       " 'abet',\n",
       " 'abey',\n",
       " 'abid',\n",
       " 'abil',\n",
       " 'abli',\n",
       " 'abod',\n",
       " 'abov',\n",
       " 'abus',\n",
       " 'acid',\n",
       " 'acut',\n",
       " 'adag',\n",
       " 'ador',\n",
       " 'agit',\n",
       " 'agre',\n",
       " 'airi',\n",
       " 'akin',\n",
       " 'alia',\n",
       " 'alik',\n",
       " 'aliv',\n",
       " 'all.',\n",
       " 'alli',\n",
       " 'alon',\n",
       " 'amaz',\n",
       " 'amen',\n",
       " 'amic',\n",
       " 'ampl',\n",
       " 'amus',\n",
       " 'anew',\n",
       " 'angl',\n",
       " 'anim',\n",
       " 'appl',\n",
       " 'arab',\n",
       " 'arch',\n",
       " 'area',\n",
       " 'argu',\n",
       " 'arid',\n",
       " 'aris',\n",
       " 'armi',\n",
       " 'aros',\n",
       " 'asid',\n",
       " 'atom',\n",
       " 'aton',\n",
       " 'aunt',\n",
       " 'aver',\n",
       " 'avoc',\n",
       " 'avow',\n",
       " 'awak',\n",
       " 'awar',\n",
       " 'away',\n",
       " 'babi',\n",
       " 'badg',\n",
       " 'bail',\n",
       " 'bait',\n",
       " 'bale',\n",
       " 'ball',\n",
       " 'band',\n",
       " 'bane',\n",
       " 'bank',\n",
       " 'bann',\n",
       " 'bare',\n",
       " 'barg',\n",
       " 'bark',\n",
       " 'barn',\n",
       " 'bart',\n",
       " 'base',\n",
       " 'basi',\n",
       " 'bate',\n",
       " 'bath',\n",
       " 'bead',\n",
       " 'beam',\n",
       " 'bean',\n",
       " 'bear',\n",
       " 'beat',\n",
       " 'beef',\n",
       " 'beer',\n",
       " 'beet',\n",
       " 'beli',\n",
       " 'bell',\n",
       " 'belt',\n",
       " 'bend',\n",
       " 'bent',\n",
       " 'best',\n",
       " 'bias',\n",
       " 'bibl',\n",
       " 'bind',\n",
       " 'bird',\n",
       " 'bite',\n",
       " 'bled',\n",
       " 'blew',\n",
       " 'bloc',\n",
       " 'blot',\n",
       " 'blow',\n",
       " 'blue',\n",
       " 'boar',\n",
       " 'boat',\n",
       " 'bodi',\n",
       " 'boer',\n",
       " 'boil',\n",
       " 'bold',\n",
       " 'bolt',\n",
       " 'bona',\n",
       " 'bond',\n",
       " 'bone',\n",
       " 'book',\n",
       " 'boon',\n",
       " 'boot',\n",
       " 'bore',\n",
       " 'born',\n",
       " 'bred',\n",
       " 'brew',\n",
       " 'brig',\n",
       " 'brow',\n",
       " 'bulk',\n",
       " 'bull',\n",
       " 'buoy',\n",
       " 'buri',\n",
       " 'burk',\n",
       " 'burn',\n",
       " 'bush',\n",
       " 'busi',\n",
       " 'butt',\n",
       " 'cabl',\n",
       " 'cadr',\n",
       " 'cage',\n",
       " 'cake',\n",
       " 'calf',\n",
       " 'calm',\n",
       " 'calv',\n",
       " 'came',\n",
       " 'camp',\n",
       " 'cane',\n",
       " 'capt',\n",
       " 'card',\n",
       " 'care',\n",
       " 'carp',\n",
       " 'cart',\n",
       " 'carv',\n",
       " 'case',\n",
       " 'cash',\n",
       " 'cask',\n",
       " 'cast',\n",
       " 'caus',\n",
       " 'ceas',\n",
       " 'cede',\n",
       " 'ceed',\n",
       " 'ceiv',\n",
       " 'cell',\n",
       " 'cent',\n",
       " 'cer-',\n",
       " 'cess',\n",
       " 'chao',\n",
       " 'chap',\n",
       " 'chop',\n",
       " 'cipl',\n",
       " 'cite',\n",
       " 'citi',\n",
       " 'clay',\n",
       " 'clip',\n",
       " 'clog',\n",
       " 'club',\n",
       " 'clue',\n",
       " 'coal',\n",
       " 'coat',\n",
       " 'code',\n",
       " 'coin',\n",
       " 'col.',\n",
       " 'cold',\n",
       " 'com-',\n",
       " 'come',\n",
       " 'con-',\n",
       " 'coni',\n",
       " 'cook',\n",
       " 'cool',\n",
       " 'cope',\n",
       " 'copi',\n",
       " 'cord',\n",
       " 'cork',\n",
       " 'corn',\n",
       " 'corp',\n",
       " 'cost',\n",
       " 'coup',\n",
       " 'cram',\n",
       " 'crew',\n",
       " 'crop',\n",
       " 'crow',\n",
       " 'crux',\n",
       " 'cull',\n",
       " 'curb',\n",
       " 'cure',\n",
       " 'curs',\n",
       " 'cwts',\n",
       " 'cycl',\n",
       " 'damn',\n",
       " 'damp',\n",
       " 'danc',\n",
       " 'dare',\n",
       " 'dark',\n",
       " 'darl',\n",
       " 'dash',\n",
       " 'data',\n",
       " 'date',\n",
       " 'dawn',\n",
       " 'day.',\n",
       " 'dead',\n",
       " 'deaf',\n",
       " 'deal',\n",
       " 'dean',\n",
       " 'dear',\n",
       " 'debt',\n",
       " 'deck',\n",
       " 'deed',\n",
       " 'deem',\n",
       " 'deep',\n",
       " 'deer',\n",
       " 'defi',\n",
       " 'denc',\n",
       " 'deni',\n",
       " 'dens',\n",
       " 'desk',\n",
       " 'diem',\n",
       " 'diet',\n",
       " 'dine',\n",
       " 'dire',\n",
       " 'dirt',\n",
       " 'dis-',\n",
       " 'dive',\n",
       " 'dock',\n",
       " 'dodg',\n",
       " 'dole',\n",
       " 'doom',\n",
       " 'door',\n",
       " 'dose',\n",
       " 'doth',\n",
       " 'drag',\n",
       " 'draw',\n",
       " 'drew',\n",
       " 'drop',\n",
       " 'drug',\n",
       " 'drum',\n",
       " 'dual',\n",
       " 'duce',\n",
       " 'duck',\n",
       " 'duct',\n",
       " 'duel',\n",
       " 'duke',\n",
       " 'duli',\n",
       " 'dull',\n",
       " 'dumb',\n",
       " 'dump',\n",
       " 'dupe',\n",
       " 'dust',\n",
       " 'duti',\n",
       " 'earl',\n",
       " 'earn',\n",
       " 'easi',\n",
       " 'east',\n",
       " 'echo',\n",
       " 'edit',\n",
       " 'educ',\n",
       " 'eleg',\n",
       " 'elev',\n",
       " 'elig',\n",
       " 'elud',\n",
       " 'eman',\n",
       " 'emin',\n",
       " 'emot',\n",
       " 'emul',\n",
       " 'ensu',\n",
       " 'envi',\n",
       " 'erad',\n",
       " 'eras',\n",
       " 'evad',\n",
       " 'evas',\n",
       " 'evid',\n",
       " 'evil',\n",
       " 'evok',\n",
       " 'exig',\n",
       " 'exil',\n",
       " 'exit',\n",
       " 'fabl',\n",
       " 'face',\n",
       " 'faci',\n",
       " 'fact',\n",
       " 'fade',\n",
       " 'fail',\n",
       " 'fain',\n",
       " 'fair',\n",
       " 'fall',\n",
       " 'fals',\n",
       " 'fame',\n",
       " 'farc',\n",
       " 'fare',\n",
       " 'farm',\n",
       " 'fast',\n",
       " 'fate',\n",
       " 'fear',\n",
       " 'feat',\n",
       " 'feed',\n",
       " 'feel',\n",
       " 'feet',\n",
       " 'fell',\n",
       " 'felt',\n",
       " 'fenc',\n",
       " 'feud',\n",
       " 'fiat',\n",
       " 'fibr',\n",
       " 'fide',\n",
       " 'file',\n",
       " 'fine',\n",
       " 'firm',\n",
       " 'fish',\n",
       " 'flag',\n",
       " 'flat',\n",
       " 'flaw',\n",
       " 'flax',\n",
       " 'fled',\n",
       " 'flog',\n",
       " 'flow',\n",
       " 'foil',\n",
       " 'fold',\n",
       " 'fond',\n",
       " 'food',\n",
       " 'fool',\n",
       " 'foot',\n",
       " 'for-',\n",
       " 'forc',\n",
       " 'ford',\n",
       " 'fore',\n",
       " 'forg',\n",
       " 'form',\n",
       " 'fort',\n",
       " 'foul',\n",
       " 'fowl',\n",
       " 'free',\n",
       " 'fuel',\n",
       " 'fund',\n",
       " 'furi',\n",
       " 'fuse',\n",
       " 'gain',\n",
       " 'gale',\n",
       " 'gall',\n",
       " 'game',\n",
       " 'gang',\n",
       " 'gaol',\n",
       " 'garb',\n",
       " 'gard',\n",
       " 'gase',\n",
       " 'gate',\n",
       " 'gaug',\n",
       " 'gave',\n",
       " 'gaze',\n",
       " 'gear',\n",
       " 'gen-',\n",
       " 'gen.',\n",
       " 'gent',\n",
       " 'germ',\n",
       " 'gift',\n",
       " 'girl',\n",
       " 'gist',\n",
       " 'glad',\n",
       " 'glow',\n",
       " 'glut',\n",
       " 'goad',\n",
       " 'goal',\n",
       " 'gold',\n",
       " 'gone',\n",
       " 'good',\n",
       " 'goos',\n",
       " 'govt',\n",
       " 'gown',\n",
       " 'grab',\n",
       " 'grew',\n",
       " 'grey',\n",
       " 'grow',\n",
       " 'guid',\n",
       " 'guis',\n",
       " 'gulf',\n",
       " 'hack',\n",
       " 'hail',\n",
       " 'hair',\n",
       " 'half',\n",
       " 'hall',\n",
       " 'halt',\n",
       " 'halv',\n",
       " 'hand',\n",
       " 'hang',\n",
       " 'hard',\n",
       " 'hare',\n",
       " 'harm',\n",
       " 'hast',\n",
       " 'hate',\n",
       " 'hath',\n",
       " 'hatr',\n",
       " 'haul',\n",
       " 'hawk',\n",
       " 'head',\n",
       " 'heal',\n",
       " 'heap',\n",
       " 'hear',\n",
       " 'heat',\n",
       " 'hedg',\n",
       " 'heed',\n",
       " 'heel',\n",
       " 'heir',\n",
       " 'held',\n",
       " 'hell',\n",
       " 'helm',\n",
       " 'help',\n",
       " 'hemp',\n",
       " 'henc',\n",
       " 'herd',\n",
       " 'hero',\n",
       " 'hide',\n",
       " 'high',\n",
       " 'hill',\n",
       " 'hilt',\n",
       " 'him.',\n",
       " 'hing',\n",
       " 'hint',\n",
       " 'hire',\n",
       " 'hive',\n",
       " 'hoar',\n",
       " 'hold',\n",
       " 'hole',\n",
       " 'holi',\n",
       " 'home',\n",
       " 'homo',\n",
       " 'hook',\n",
       " 'hoot',\n",
       " 'hope',\n",
       " 'hord',\n",
       " 'horn',\n",
       " 'hors',\n",
       " 'host',\n",
       " 'hour',\n",
       " 'hous',\n",
       " 'how-',\n",
       " 'howl',\n",
       " 'huge',\n",
       " 'hulk',\n",
       " 'hull',\n",
       " 'hung',\n",
       " 'hunt',\n",
       " 'hurl',\n",
       " 'hurt',\n",
       " 'hush',\n",
       " 'hust',\n",
       " 'hymn',\n",
       " 'idea',\n",
       " 'idol',\n",
       " 'imag',\n",
       " 'imbu',\n",
       " 'imit',\n",
       " 'inch',\n",
       " 'inde',\n",
       " 'inim',\n",
       " 'inop',\n",
       " 'inst',\n",
       " 'iota',\n",
       " 'ipso',\n",
       " 'iron',\n",
       " 'isol',\n",
       " 'issu',\n",
       " 'item',\n",
       " 'jail',\n",
       " 'ject',\n",
       " 'jeer',\n",
       " 'jest',\n",
       " 'join',\n",
       " 'joke',\n",
       " 'judg',\n",
       " 'juic',\n",
       " 'jump',\n",
       " 'jure',\n",
       " 'juri',\n",
       " 'keen',\n",
       " 'kept',\n",
       " 'kick',\n",
       " 'kill',\n",
       " 'kind',\n",
       " 'king',\n",
       " 'kirk',\n",
       " 'kiss',\n",
       " 'knee',\n",
       " 'knew',\n",
       " 'knit',\n",
       " 'knot',\n",
       " 'know',\n",
       " 'lace',\n",
       " 'lach',\n",
       " 'lack',\n",
       " 'lade',\n",
       " 'ladi',\n",
       " 'laid',\n",
       " 'lain',\n",
       " 'lake',\n",
       " 'lakh',\n",
       " 'lamb',\n",
       " 'lame',\n",
       " 'lamp',\n",
       " 'land',\n",
       " 'lane',\n",
       " 'laps',\n",
       " 'larg',\n",
       " 'lash',\n",
       " 'late',\n",
       " 'laud',\n",
       " 'lave',\n",
       " 'law.',\n",
       " 'lazi',\n",
       " 'lead',\n",
       " 'leaf',\n",
       " 'leak',\n",
       " 'lean',\n",
       " 'leap',\n",
       " 'leas',\n",
       " 'leav',\n",
       " 'ledg',\n",
       " 'left',\n",
       " 'lend',\n",
       " 'lent',\n",
       " 'lest',\n",
       " 'leve',\n",
       " 'levi',\n",
       " 'liar',\n",
       " 'lien',\n",
       " 'lieu',\n",
       " 'liev',\n",
       " 'life',\n",
       " 'lift',\n",
       " 'like',\n",
       " 'limb',\n",
       " 'lime',\n",
       " 'line',\n",
       " 'link',\n",
       " 'lion',\n",
       " 'list',\n",
       " 'live',\n",
       " 'load',\n",
       " 'loaf',\n",
       " 'loan',\n",
       " 'loav',\n",
       " 'lock',\n",
       " 'lodg',\n",
       " 'loft',\n",
       " 'long',\n",
       " 'look',\n",
       " 'loom',\n",
       " 'loos',\n",
       " 'loot',\n",
       " 'lord',\n",
       " 'lose',\n",
       " 'loss',\n",
       " 'lost',\n",
       " 'loth',\n",
       " 'loud',\n",
       " 'love',\n",
       " 'luck',\n",
       " 'lull',\n",
       " 'lump',\n",
       " 'lung',\n",
       " 'lure',\n",
       " 'lurk',\n",
       " 'lust',\n",
       " 'maid',\n",
       " 'mail',\n",
       " 'maim',\n",
       " 'main',\n",
       " 'maiz',\n",
       " 'male',\n",
       " 'malt',\n",
       " 'man.',\n",
       " 'mani',\n",
       " 'mans',\n",
       " 'mare',\n",
       " 'mark',\n",
       " 'mart',\n",
       " 'mask',\n",
       " 'mass',\n",
       " 'mast',\n",
       " 'mate',\n",
       " 'mayb',\n",
       " 'mea-',\n",
       " 'meal',\n",
       " 'mean',\n",
       " 'meat',\n",
       " 'meed',\n",
       " 'meek',\n",
       " 'meet',\n",
       " 'melt',\n",
       " 'mem-',\n",
       " 'men.',\n",
       " 'mend',\n",
       " 'ment',\n",
       " 'mere',\n",
       " 'merg',\n",
       " 'mesh',\n",
       " 'mess',\n",
       " 'mete',\n",
       " 'mild',\n",
       " 'mile',\n",
       " 'milk',\n",
       " 'mind',\n",
       " 'mint',\n",
       " 'misl',\n",
       " 'miss',\n",
       " 'mist',\n",
       " 'moan',\n",
       " 'moat',\n",
       " 'mock',\n",
       " 'mode',\n",
       " 'moni',\n",
       " ...]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(idmap, key=lambda i: len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
